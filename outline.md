# introduction

1. meaning acquisition is a (very) hard problem
2. theorists have often proposed that meaning acqusition takes place with a language of thought
   1. meanings are composed of semantic primitives
   2. the "acquisition" taking place is the composition of these primitives
3. but then, language models have been generally very successful and nice
4. these guys use distributional representations to acquire meaning
   1. meanings are encoded by their context
   2. the acquisition taking place is the learning of these (contextual) embeddings
5. because language models are successful, we wonder: how is meaning actually learned?
6. to study this, i focus on *temporal connectives*
   1. why? they have very specific learning trajectories
7. basically: do LMs learn the meanings of connectives in a human-like way? because LOT models definitely do.